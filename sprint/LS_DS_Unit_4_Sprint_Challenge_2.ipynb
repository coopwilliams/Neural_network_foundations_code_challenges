{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2*\n",
    "\n",
    "# Sprint Challenge - Neural Network Foundations\n",
    "\n",
    "Table of Problems\n",
    "\n",
    "1. [Defining Neural Networks](#Q1)\n",
    "2. [Chocolate Gummy Bears](#Q2)\n",
    "    - Perceptron\n",
    "    - Multilayer Perceptron\n",
    "4. [Keras MMP](#Q3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Q1\"></a>\n",
    "## 1. Define the following terms:\n",
    "\n",
    "- **Neuron:**\n",
    "A node in a neural network. This can be a simple function that computes an activation function on the weighted sum of all inputs. In more advnaced networks, a neuron can have memory and state.\n",
    "- **Input Layer:**\n",
    "The layer of nodes in a  neural network that takes the input data for training and prediction. The number of nodes is equal to the number of features in the learning problem.\n",
    "- **Hidden Layer:**\n",
    "A layer of nodes in a neural network that is between the input and output layers, which computes a non-linear function on its inputs and passes the result to the next layer. The network can theoretically have any number of these layers, and more layers allows the network to learn more levels of abstraction.\n",
    "- **Output Layer:**\n",
    "The layer of nodes in a neural network that computes the output on the result of all previous layers. This is typically an affine transformation on the output of hidden layers, producing numbers that we recognize as the machine's \"answer\" to a problem.\n",
    "- **Activation:**\n",
    "The activation function regularizes the output of a node in a hidden layer. Only non-linear activation functions can be used to solve non-trivial problems. These functions typically contrain the output of a node to a given range, in order to prevent outputs from exploding or disappearing. They are also smooth and continuously differentiable so that they can be used in gradient descent, though there are exceptions like ReLU. \n",
    "- **Backpropagation:**\n",
    "A method of a neural network that modifies its weights after each training step. It computes the gradient of the network's loss function with respect to each weight.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chocolate Gummy Bears <a id=\"Q2\"></a>\n",
    "\n",
    "Right now, you're probably thinking, \"yuck, who the hell would eat that?\". Great question. Your candy company wants to know too. And you thought I was kidding about the [Chocolate Gummy Bears](https://nuts.com/chocolatessweets/gummies/gummy-bears/milk-gummy-bears.html?utm_source=google&utm_medium=cpc&adpos=1o1&gclid=Cj0KCQjwrfvsBRD7ARIsAKuDvMOZrysDku3jGuWaDqf9TrV3x5JLXt1eqnVhN0KM6fMcbA1nod3h8AwaAvWwEALw_wcB). \n",
    "\n",
    "Let's assume that a candy company has gone out and collected information on the types of Halloween candy kids ate. Our candy company wants to predict the eating behavior of witches, warlocks, and ghosts -- aka costumed kids. They shared a sample dataset with us. Each row represents a piece of candy that a costumed child was presented with during \"trick\" or \"treat\". We know if the candy was `chocolate` (or not chocolate) or `gummy` (or not gummy). Your goal is to predict if the costumed kid `ate` the piece of candy. \n",
    "\n",
    "If both chocolate and gummy equal one, you've got a chocolate gummy bear on your hands!?!?!\n",
    "![Chocolate Gummy Bear](https://ed910ae2d60f0d25bcb8-80550f96b5feb12604f4f720bfefb46d.ssl.cf1.rackcdn.com/3fb630c04435b7b5-2leZuM7_-zoom.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "candy = pd.read_csv('chocolate_gummy_bears.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chocolate</th>\n",
       "      <th>gummy</th>\n",
       "      <th>ate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    chocolate  gummy  ate\n",
       "0           0      1    1\n",
       "1           1      0    1\n",
       "2           0      1    1\n",
       "3           0      0    0\n",
       "4           1      1    0\n",
       "5           0      1    1\n",
       "6           0      0    0\n",
       "7           1      1    0\n",
       "8           0      0    0\n",
       "9           0      0    0\n",
       "10          0      1    1\n",
       "11          1      1    0\n",
       "12          0      0    0\n",
       "13          0      1    1\n",
       "14          0      1    1\n",
       "15          1      0    0\n",
       "16          1      0    1\n",
       "17          0      1    1\n",
       "18          1      1    0\n",
       "19          0      1    1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candy.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron\n",
    "\n",
    "To make predictions on the `candy` dataframe. Build and train a Perceptron using numpy. Your target column is `ate` and your features: `chocolate` and `gummy`. Do not do any feature engineering. :P\n",
    "\n",
    "Once you've trained your model, report your accuracy. You will not be able to achieve more than ~50% with the simple perceptron. Explain why you could not achieve a higher accuracy with the *simple perceptron* architecture, because it's possible to achieve ~95% accuracy on this dataset. Provide your answer in markdown (and *optional* data anlysis code) after your perceptron implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# Start your candy perceptron here\n",
    "\n",
    "X = candy[['chocolate', 'gummy']].values\n",
    "y = candy['ate'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.reshape(10000, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: RuntimeWarning: overflow encountered in exp\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4859.0 wrong out of 10000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    sx = sigmoid(x)\n",
    "    return sx * (1-sx)\n",
    "\n",
    "weights = 2 * np.random.random((2, 1)) - 1\n",
    "\n",
    "for i in range(1000):\n",
    "\n",
    "    weighted_sum = np.dot(X, weights)\n",
    "    activated_output = sigmoid(weighted_sum)\n",
    "    error = y - activated_output\n",
    "    adjustments = error * sigmoid_derivative(activated_output)\n",
    "    weights += np.dot(X.T, adjustments)\n",
    "\n",
    "# error\n",
    "# wrong = error[error==1].sum()\n",
    "print(error[error==1].sum(), \"wrong out of\", error.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer <a id=\"Q3\"></a>\n",
    "\n",
    "This problem is just an XOR gate, which requires a nonlinear activation function to solve. This simple perceptron is simply finding a regression line:\n",
    "\n",
    "y = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron <a id=\"Q3\"></a>\n",
    "\n",
    "Using the sample candy dataset, implement a Neural Network Multilayer Perceptron class that uses backpropagation to update the network's weights. Your Multilayer Perceptron should be implemented in Numpy. \n",
    "Your network must have one hidden layer.\n",
    "\n",
    "Once you've trained your model, report your accuracy. Explain why your MLP's performance is considerably better than your simple perceptron's on the candy dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "##### Your Code Here #####\n",
    "class MLP:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize. Adding 4 nodes to the hidden layer because that's the square of the number of features.\"\"\"\n",
    "        self.inputs = 2\n",
    "        self.hiddenNodes = 4\n",
    "        self.outputNodes = 1\n",
    "        self.weights1 = np.random.rand(self.inputs, self.hiddenNodes)\n",
    "        self.weights2 = np.random.rand(self.hiddenNodes, self.outputNodes)\n",
    "        \n",
    "    def sigmoid(self, s):\n",
    "        return 1 / (1 + np.exp(-s))\n",
    "    \n",
    "    def sigmoidPrime(self, s):\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    def feed_forward(self, X):\n",
    "        \"\"\"\n",
    "        Calculate the NN inference using feed forward.\n",
    "        aka \"predict\"\n",
    "        \"\"\"\n",
    "        self.hidden_sum = np.dot(X, self.weights1)\n",
    "        self.activated_hidden = self.sigmoid(self.hidden_sum)\n",
    "        self.output_sum = np.dot(self.activated_hidden, self.weights2)\n",
    "        self.activated_output = self.sigmoid(self.output_sum)\n",
    "    \n",
    "        return self.activated_output\n",
    "        \n",
    "    def backward(self, X,y,o):\n",
    "        \"\"\"\n",
    "        Backpropagate through the network\n",
    "        \"\"\"\n",
    "        self.o_error = y - o\n",
    "        self.o_delta = self.o_error * self.sigmoidPrime(o)\n",
    "        self.z2_error = self.o_delta.dot(self.weights2.T)\n",
    "        self.z2_delta = self.z2_error * self.sigmoidPrime(self.activated_hidden)\n",
    "        self.weights1 += X.T.dot(self.z2_delta)\n",
    "        self.weights2 += self.activated_hidden.T.dot(self.o_delta)\n",
    "        \n",
    "\n",
    "    def train(self, X, y):\n",
    "        o = self.feed_forward(X)\n",
    "        self.backward(X,y,o)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "perry = MLP()\n",
    "for i in range(20000):\n",
    "    perry.train(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error: 0.1259000644530807\n"
     ]
    }
   ],
   "source": [
    "y_pred = perry.feed_forward(X)\n",
    "\n",
    "print(\"error:\", (y - y_pred).sum() / y.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coop: I'm not exactly sure why it hasn't reached 95% accuracy yet. I'll try the implementation from Goodfellow et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XOR_MLP():\n",
    "    def __init__(self):\n",
    "        \"\"\"CLosed-form weights and bias\"\"\"\n",
    "        self.weights1 = np.array([[1, 1], [1, 1]])\n",
    "        self.weights2 = np.array([[1], [-2]])\n",
    "        self.bias = np.array([[0, -1]])\n",
    "    def relu(self, X):\n",
    "        \"\"\"numpy relu\"\"\"\n",
    "        X[X<0] = 0\n",
    "        return X\n",
    "    def predict(self, X):\n",
    "        \"\"\"predict XOR\"\"\"\n",
    "        output = np.dot(X, self.weights1)\n",
    "        activated_output = self.relu(output + np.repeat(zorro.bias, output.shape[0], axis=0))\n",
    "        return np.dot(activated_output, self.weights2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "zorro = XOR_MLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0, -1],\n",
       "       [ 0, -1],\n",
       "       [ 0, -1]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.repeat(zorro.bias, 3, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       ...,\n",
       "       [1],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = zorro.predict(X)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error: 0.002\n"
     ]
    }
   ],
   "source": [
    "print(\"error:\", (y - y_pred).sum() / y.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it, a closed-form solution. The weighted input + bias has changed the nonlinear inputs to a linear combination of vectors. The ReLU corrects for the negative elements and the second set of weights produces our expected output. No backpropagation necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P.S. Don't try candy gummy bears. They're disgusting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Keras MMP <a id=\"Q3\"></a>\n",
    "\n",
    "Implement a Multilayer Perceptron architecture of your choosing using the Keras library. Train your model and report its baseline accuracy. Then hyperparameter tune at least two parameters and report your model's accuracy.\n",
    "Use the Heart Disease Dataset (binary classification)\n",
    "Use an appropriate loss function for a binary classification task\n",
    "Use an appropriate activation function on the final layer of your network.\n",
    "Train your model using verbose output for ease of grading.\n",
    "Use GridSearchCV or RandomSearchCV to hyperparameter tune your model. (for at least two hyperparameters)\n",
    "When hyperparameter tuning, show you work by adding code cells for each new experiment.\n",
    "Report the accuracy for each combination of hyperparameters as you test them so that we can easily see which resulted in the highest accuracy.\n",
    "You must hyperparameter tune at least 3 parameters in order to get a 3 on this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(303, 14)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv')\n",
    "df = df.sample(frac=1)\n",
    "print(df.shape)\n",
    "values = df.values\n",
    "# values[:, 13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_33\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_73 (Dense)             (None, 13)                182       \n",
      "_________________________________________________________________\n",
      "dense_74 (Dense)             (None, 13)                182       \n",
      "_________________________________________________________________\n",
      "dense_75 (Dense)             (None, 13)                182       \n",
      "_________________________________________________________________\n",
      "dense_76 (Dense)             (None, 13)                182       \n",
      "_________________________________________________________________\n",
      "dense_77 (Dense)             (None, 1)                 14        \n",
      "=================================================================\n",
      "Total params: 742\n",
      "Trainable params: 742\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Best: 0.6798680027325948 using {'batch_size': 10, 'epochs': 50}\n",
      "Means: 0.6798680027325948, Stdev: 0.11152966600297419 with: {'batch_size': 10, 'epochs': 50}\n",
      "Means: 0.455445537964503, Stdev: 0.01400212435493574 with: {'batch_size': 10, 'epochs': 100}\n",
      "Means: 0.6006600658098856, Stdev: 0.11152968422881264 with: {'batch_size': 20, 'epochs': 50}\n",
      "Means: 0.5313531359036764, Stdev: 0.12135167884767872 with: {'batch_size': 20, 'epochs': 100}\n"
     ]
    }
   ],
   "source": [
    "# from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "X = values[:,0:13]\n",
    "y = values[:,13]\n",
    "\n",
    "inputs = X.shape[1]\n",
    "epochs = 50\n",
    "batch_size = 10\n",
    "hidden_size = 13\n",
    "\n",
    "def my_model():\n",
    "    heart = Sequential()\n",
    "    heart.add(Dense(13, activation='relu', input_dim=13))\n",
    "    heart.add(Dense(hidden_size, activation='sigmoid'))\n",
    "    heart.add(Dense(hidden_size, activation='sigmoid'))\n",
    "    heart.add(Dense(hidden_size, activation='sigmoid'))\n",
    "    heart.add(Dense(1))\n",
    "    heart.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    print(heart.summary())\n",
    "    return heart\n",
    "\n",
    "heart_model = KerasClassifier(build_fn=my_model, verbose=0)\n",
    "\n",
    "param_grid = {'batch_size':[10, 20],\n",
    "              'epochs':[50, 100]}\n",
    "\n",
    "grid = GridSearchCV(estimator=heart_model, param_grid=param_grid, n_jobs=-1)\n",
    "grid_result = grid.fit(X, y)\n",
    "\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_34\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_78 (Dense)             (None, 13)                182       \n",
      "_________________________________________________________________\n",
      "dense_79 (Dense)             (None, 26)                364       \n",
      "_________________________________________________________________\n",
      "dense_80 (Dense)             (None, 26)                702       \n",
      "_________________________________________________________________\n",
      "dense_81 (Dense)             (None, 26)                702       \n",
      "_________________________________________________________________\n",
      "dense_82 (Dense)             (None, 1)                 27        \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,977\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Best: 0.6270627081394196 using {'batch_size': 20, 'epochs': 100}\n",
      "Means: 0.49174917737642926, Stdev: 0.04596828797941037 with: {'batch_size': 10, 'epochs': 50}\n",
      "Means: 0.6006600757439932, Stdev: 0.12558612235247857 with: {'batch_size': 10, 'epochs': 100}\n",
      "Means: 0.5643564363320669, Stdev: 0.14752143599814496 with: {'batch_size': 20, 'epochs': 50}\n",
      "Means: 0.6270627081394196, Stdev: 0.10765350555337773 with: {'batch_size': 20, 'epochs': 100}\n"
     ]
    }
   ],
   "source": [
    "# from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "X = values[:,0:13]\n",
    "y = values[:,13]\n",
    "\n",
    "inputs = X.shape[1]\n",
    "epochs = 50\n",
    "batch_size = 10\n",
    "hidden_size = 26\n",
    "\n",
    "def my_model():\n",
    "    heart = Sequential()\n",
    "    heart.add(Dense(13, activation='relu', input_dim=13))\n",
    "    heart.add(Dense(hidden_size, activation='sigmoid'))\n",
    "    heart.add(Dense(hidden_size, activation='sigmoid'))\n",
    "    heart.add(Dense(hidden_size, activation='sigmoid'))\n",
    "    heart.add(Dense(1))\n",
    "    heart.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    print(heart.summary())\n",
    "    return heart\n",
    "\n",
    "heart_model = KerasClassifier(build_fn=my_model, verbose=0)\n",
    "\n",
    "param_grid = {'batch_size':[10, 20],\n",
    "              'epochs':[50, 100]}\n",
    "\n",
    "grid = GridSearchCV(estimator=heart_model, param_grid=param_grid, n_jobs=-1)\n",
    "grid_result = grid.fit(X, y)\n",
    "\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_36\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_88 (Dense)             (None, 13)                182       \n",
      "_________________________________________________________________\n",
      "dense_89 (Dense)             (None, 39)                546       \n",
      "_________________________________________________________________\n",
      "dense_90 (Dense)             (None, 39)                1560      \n",
      "_________________________________________________________________\n",
      "dense_91 (Dense)             (None, 39)                1560      \n",
      "_________________________________________________________________\n",
      "dense_92 (Dense)             (None, 1)                 40        \n",
      "=================================================================\n",
      "Total params: 3,888\n",
      "Trainable params: 3,888\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Best: 0.7161716222763062 using {'batch_size': 30, 'epochs': 200}\n",
      "Means: 0.455445537964503, Stdev: 0.01400212435493574 with: {'batch_size': 15, 'epochs': 50}\n",
      "Means: 0.5643564462661743, Stdev: 0.11659091502126338 with: {'batch_size': 15, 'epochs': 100}\n",
      "Means: 0.5874587496121725, Stdev: 0.1314343833221585 with: {'batch_size': 15, 'epochs': 200}\n",
      "Means: 0.6534653504689535, Stdev: 0.053011156315262195 with: {'batch_size': 20, 'epochs': 50}\n",
      "Means: 0.5577557782332102, Stdev: 0.13822041539994606 with: {'batch_size': 20, 'epochs': 100}\n",
      "Means: 0.49174917737642926, Stdev: 0.04596828797941037 with: {'batch_size': 20, 'epochs': 200}\n",
      "Means: 0.5511551102002462, Stdev: 0.0849153811215931 with: {'batch_size': 30, 'epochs': 50}\n",
      "Means: 0.455445537964503, Stdev: 0.01400212435493574 with: {'batch_size': 30, 'epochs': 100}\n",
      "Means: 0.7161716222763062, Stdev: 0.13750933754114447 with: {'batch_size': 30, 'epochs': 200}\n"
     ]
    }
   ],
   "source": [
    "# from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "X = values[:,0:13]\n",
    "y = values[:,13]\n",
    "\n",
    "inputs = X.shape[1]\n",
    "epochs = 50\n",
    "batch_size = 10\n",
    "hidden_size = 39\n",
    "\n",
    "def my_model():\n",
    "    heart = Sequential()\n",
    "    heart.add(Dense(13, activation='relu', input_dim=13))\n",
    "    heart.add(Dense(hidden_size, activation='sigmoid'))\n",
    "    heart.add(Dense(hidden_size, activation='sigmoid'))\n",
    "    heart.add(Dense(hidden_size, activation='sigmoid'))\n",
    "    heart.add(Dense(1))\n",
    "    heart.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    print(heart.summary())\n",
    "    return heart\n",
    "\n",
    "heart_model = KerasClassifier(build_fn=my_model, verbose=0)\n",
    "\n",
    "param_grid = {'batch_size':[15, 20, 30],\n",
    "              'epochs':[50, 100, 200]}\n",
    "\n",
    "grid = GridSearchCV(estimator=heart_model, param_grid=param_grid, n_jobs=-1)\n",
    "grid_result = grid.fit(X, y)\n",
    "\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_37\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_93 (Dense)             (None, 13)                182       \n",
      "_________________________________________________________________\n",
      "dense_94 (Dense)             (None, 39)                546       \n",
      "_________________________________________________________________\n",
      "dense_95 (Dense)             (None, 39)                1560      \n",
      "_________________________________________________________________\n",
      "dense_96 (Dense)             (None, 39)                1560      \n",
      "_________________________________________________________________\n",
      "dense_97 (Dense)             (None, 1)                 40        \n",
      "=================================================================\n",
      "Total params: 3,888\n",
      "Trainable params: 3,888\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Best: 0.7953795393308004 using {'batch_size': 20, 'epochs': 200}\n",
      "Means: 0.7227722803751627, Stdev: 0.04042063510541671 with: {'batch_size': 20, 'epochs': 100}\n",
      "Means: 0.7953795393308004, Stdev: 0.025986816922028384 with: {'batch_size': 20, 'epochs': 200}\n",
      "Means: 0.7392739454905192, Stdev: 0.028390503971451875 with: {'batch_size': 30, 'epochs': 100}\n",
      "Means: 0.7953795393308004, Stdev: 0.0373389608159633 with: {'batch_size': 30, 'epochs': 200}\n"
     ]
    }
   ],
   "source": [
    "# from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "X = values[:,0:13]\n",
    "y = values[:,13]\n",
    "\n",
    "inputs = X.shape[1]\n",
    "epochs = 50\n",
    "batch_size = 10\n",
    "hidden_size = 39\n",
    "\n",
    "def my_model():\n",
    "    heart = Sequential()\n",
    "    heart.add(Dense(13, activation='relu', input_dim=13))\n",
    "    heart.add(Dense(hidden_size, activation='sigmoid'))\n",
    "    heart.add(Dense(hidden_size, activation='sigmoid'))\n",
    "    heart.add(Dense(hidden_size, activation='sigmoid'))\n",
    "    heart.add(Dense(1))\n",
    "    heart.compile(loss='hinge', optimizer='adam', metrics=['accuracy'])\n",
    "    print(heart.summary())\n",
    "    return heart\n",
    "\n",
    "heart_model = KerasClassifier(build_fn=my_model, verbose=0)\n",
    "\n",
    "param_grid = {'batch_size':[20, 30],\n",
    "              'epochs':[100, 200]}\n",
    "\n",
    "grid = GridSearchCV(estimator=heart_model, param_grid=param_grid, n_jobs=-1)\n",
    "grid_result = grid.fit(X, y)\n",
    "\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_38\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_98 (Dense)             (None, 13)                182       \n",
      "_________________________________________________________________\n",
      "dense_99 (Dense)             (None, 39)                546       \n",
      "_________________________________________________________________\n",
      "dense_100 (Dense)            (None, 39)                1560      \n",
      "_________________________________________________________________\n",
      "dense_101 (Dense)            (None, 39)                1560      \n",
      "_________________________________________________________________\n",
      "dense_102 (Dense)            (None, 1)                 40        \n",
      "=================================================================\n",
      "Total params: 3,888\n",
      "Trainable params: 3,888\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Best: 0.7854785521825155 using {'batch_size': 20, 'epochs': 200}\n",
      "Means: 0.7557755907376608, Stdev: 0.028390503971451875 with: {'batch_size': 20, 'epochs': 100}\n",
      "Means: 0.7854785521825155, Stdev: 0.06067581132594037 with: {'batch_size': 20, 'epochs': 200}\n",
      "Means: 0.7128712932268778, Stdev: 0.029147743940335084 with: {'batch_size': 30, 'epochs': 100}\n",
      "Means: 0.7722772359848022, Stdev: 0.03523787151819811 with: {'batch_size': 30, 'epochs': 200}\n"
     ]
    }
   ],
   "source": [
    "# from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "X = values[:,0:13]\n",
    "y = values[:,13]\n",
    "\n",
    "inputs = X.shape[1]\n",
    "epochs = 50\n",
    "batch_size = 10\n",
    "hidden_size = 39\n",
    "\n",
    "def my_model():\n",
    "    heart = Sequential()\n",
    "    heart.add(Dense(13, activation='relu', input_dim=13))\n",
    "    heart.add(Dense(hidden_size, activation='sigmoid'))\n",
    "    heart.add(Dense(hidden_size, activation='sigmoid'))\n",
    "    heart.add(Dense(hidden_size, activation='sigmoid'))\n",
    "    heart.add(Dense(1))\n",
    "    heart.compile(loss='hinge', optimizer='adam', metrics=['accuracy'])\n",
    "    print(heart.summary())\n",
    "    return heart\n",
    "\n",
    "heart_model = KerasClassifier(build_fn=my_model, verbose=0)\n",
    "\n",
    "param_grid = {'batch_size':[20, 30],\n",
    "              'epochs':[100, 200]}\n",
    "\n",
    "grid = GridSearchCV(estimator=heart_model, param_grid=param_grid, cv=3, n_jobs=-1)\n",
    "grid_result = grid.fit(X, y)\n",
    "\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting results\n",
    "\n",
    "Here I tuned 3 different hyperparameters: batch_size, epochs, and the number of nodes in a hidden layer. That last hyperparameter I tuned by increments of 13 (the number of features). This seemed to yield better results as long as batch_size was able to increase alongside it. "
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "nteract": {
   "version": "0.15.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
